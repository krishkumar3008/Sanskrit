{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad8ef00d"
   },
   "source": [
    "# Task\n",
    "Build a CPU-based Retrieval-Augmented Generation (RAG) system for Sanskrit documents, starting with loading and preprocessing the document from `/content/Rag-docs.docx`, and ultimately providing a comprehensive technical report of the system's architecture, performance, and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (6.0.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
      "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m245.8/253.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: python-docx\n",
      "Successfully installed python-docx-1.2.0\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded document: /content/Rag-docs.docx\n",
      "Extracted 9103 characters.\n",
      "\n",
      "--- First 500 characters of extracted text ---\n",
      "मूर्खभृत्यस्य\n",
      "\n",
      "\"अरे शंखनाद, गच्छापणम्, शर्कराम् आनय ।\" इति स्वभृत्यम् शंखनादम् गोवर्धनदासः आदिशति । ततः शंखनादः आपणम् गच्छति, शर्कराम् जीर्णे वस्त्रे न्यस्यति च । तस्मात् जीर्णवस्त्रात् मार्गे एव सर्वापि शर्करा स्त्रवति । ततः गोवर्धनदासः कोपेन शंखनादम् वदति, \"अरे मूढ, कुत्रास्ति शर्करा ? शर्करादिकम् एवम् जीर्णेन वस्त्रेण न एवानयन्ति कदापि । इतःपरम् किमपि वस्तुजातम् दृढायाम् सन्चिकायाम् निक्षिप्य आनय च \" इति । अत्रान्तरे गोवर्धनदासस्य पुत्रः \"श्वानशावकम् आनय\" इति शंखनादम् आदिशति । आज्ञापालकः शंखन\n",
      "\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "\n",
    "docx_file_path = '/content/Rag-docs.docx'\n",
    "\n",
    "try:\n",
    "    document = Document(docx_file_path)\n",
    "    print(f\"Successfully loaded document: {docx_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading document: {e}\")\n",
    "    document = None\n",
    "\n",
    "plaintext_content = []\n",
    "if document:\n",
    "    for para in document.paragraphs:\n",
    "        plaintext_content.append(para.text)\n",
    "    extracted_text = '\\n'.join(plaintext_content)\n",
    "    print(f\"Extracted {len(extracted_text)} characters.\")\n",
    "    print(\"\\n--- First 500 characters of extracted text ---\")\n",
    "    print(extracted_text[:500])\n",
    "    print(\"\\n---------------------------------------------\")\n",
    "else:\n",
    "    extracted_text = \"\"\n",
    "    print(\"No document loaded, so no text to extract.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 9103\n",
      "Cleaned text length: 8988\n",
      "\n",
      "--- First 200 characters of cleaned text ---\n",
      "मूर्खभृत्यस्य\n",
      "\"अरे शंखनाद, गच्छापणम्, शर्कराम् आनय ।\" इति स्वभृत्यम् शंखनादम् गोवर्धनदासः आदिशति । ततः शंखनादः आपणम् गच्छति, शर्कराम् जीर्णे वस्त्रे न्यस्यति च । तस्मात् जीर्णवस्त्रात् मार्गे एव सर्वा\n",
      "-------------------------------------------\n",
      "\n",
      "Chunk size: 500 characters\n",
      "Chunk overlap: 50 characters\n",
      "\n",
      "Total number of chunks created: 20\n",
      "\n",
      "--- First 3 chunks ---\n",
      "\n",
      "Chunk 1 (length: 500):\n",
      "मूर्खभृत्यस्य\n",
      "\"अरे शंखनाद, गच्छापणम्, शर्कराम् आनय ।\" इति स्वभृत्यम् शंखनादम् गोवर्धनदासः आदिशति । ततः शंखनादः आपणम् गच्छति, शर्कराम् जीर्णे वस्त्रे न्यस्यति च । तस्मात् जीर्णवस्त्रात् मार्गे एव सर्वा...\n",
      "\n",
      "Chunk 2 (length: 500):\n",
      "शावकम् आनय\" इति शंखनादम् आदिशति । आज्ञापालकः शंखनादः श्वानशावकम् सन्चिकायाम् क्षिपति, सन्चिकाम् वस्त्रेण आच्छादयति च । तेन शावकस्य श्वासः रुध्दः भवति । सः च श्वानशावकः पञ्चत्वम् गच्छति । तदा गोवर्धनदा...\n",
      "\n",
      "Chunk 3 (length: 500):\n",
      " पात्रम् लुठति । पात्रात् दुग्धम् सर्वत्र प्रवहति । तेन हताशः गोवर्धनदासः तम् वदति \"भो महापंडित, अपसर; कृष्णम् भवतु ते मुखम् ” इति । तदा आज्ञापालकः शंखनादः बहिः गच्छति कज्जलेन मुखम् लिम्पति । तेन तस्य...\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "cleaned_sanskrit_text = re.sub(r'\\n{2,}', '\\n', extracted_text)\n",
    "cleaned_sanskrit_text = re.sub(r'\\s{2,}', ' ', cleaned_sanskrit_text)\n",
    "cleaned_sanskrit_text = cleaned_sanskrit_text.strip()\n",
    "\n",
    "print(f\"Original text length: {len(extracted_text)}\")\n",
    "print(f\"Cleaned text length: {len(cleaned_sanskrit_text)}\")\n",
    "print(\"\\n--- First 200 characters of cleaned text ---\")\n",
    "print(cleaned_sanskrit_text[:200])\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "chunk_size = 500\n",
    "chunk_overlap = 50\n",
    "print(f\"\\nChunk size: {chunk_size} characters\")\n",
    "print(f\"Chunk overlap: {chunk_overlap} characters\")\n",
    "\n",
    "sanskrit_text_chunks = []\n",
    "text_length = len(cleaned_sanskrit_text)\n",
    "i = 0\n",
    "while i < text_length:\n",
    "    end_index = min(i + chunk_size, text_length)\n",
    "    chunk = cleaned_sanskrit_text[i:end_index]\n",
    "    sanskrit_text_chunks.append(chunk)\n",
    "    if end_index == text_length:\n",
    "        break\n",
    "    i += (chunk_size - chunk_overlap)\n",
    "\n",
    "print(f\"\\nTotal number of chunks created: {len(sanskrit_text_chunks)}\")\n",
    "\n",
    "print(\"\\n--- First 3 chunks ---\")\n",
    "for j, chunk in enumerate(sanskrit_text_chunks[:3]):\n",
    "    print(f\"\\nChunk {j+1} (length: {len(chunk)}):\\n{chunk[:200]}...\") \n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16abc82c0a0042a293d0e95c6fc9b94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9ef045879d4b18ad7386410ff23c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa2f93eef054c0e88a688488b3b4ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27300ed6fccc40ef8490e158fb824ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20e404bd1014d0c98c14739d5e91b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff14c17c2e94f1daeff657f864a37d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ca736a0b1d45bd8e7efec122b299b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/402 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8161b3ee2754a47afa7e15b155d2558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0653372cfe964ab486f88f7ee4479757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced72586ff3740f282062c3319d9a87f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4b4711ecc543b7a8a6227d97b486a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded embedding model: paraphrase-multilingual-mpnet-base-v2 on cpu\n",
      "\n",
      "Generating embeddings for 3 sample chunks...\n",
      "Shape of generated embeddings: torch.Size([3, 768])\n",
      "Embeddings generated successfully for sample chunks.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    embedding_model = SentenceTransformer(model_name, device=device)\n",
    "    print(f\"Successfully loaded embedding model: {model_name} on {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading embedding model: {e}\")\n",
    "    embedding_model = None\n",
    "\n",
    "if embedding_model:\n",
    "    sample_chunks = sanskrit_text_chunks[:3]\n",
    "    print(f\"\\nGenerating embeddings for {len(sample_chunks)} sample chunks...\")\n",
    "\n",
    "    sample_embeddings = embedding_model.encode(sample_chunks, convert_to_tensor=True)\n",
    "\n",
    "    print(f\"Shape of generated embeddings: {sample_embeddings.shape}\")\n",
    "    print(\"Embeddings generated successfully for sample chunks.\")\n",
    "else:\n",
    "    print(\"Embedding model not loaded, skipping embedding generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
      "Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.13.2\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 768\n",
      "Generating embeddings for 20 chunks...\n",
      "Shape of all_chunk_embeddings: torch.Size([20, 768])\n",
      "FAISS index created with dimension 768.\n",
      "Number of vectors in the FAISS index: 20\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "embedding_dimension = embedding_model.get_sentence_embedding_dimension()\n",
    "print(f\"Embedding dimension: {embedding_dimension}\")\n",
    "\n",
    "print(f\"Generating embeddings for {len(sanskrit_text_chunks)} chunks...\")\n",
    "all_chunk_embeddings = embedding_model.encode(sanskrit_text_chunks, convert_to_tensor=True)\n",
    "print(f\"Shape of all_chunk_embeddings: {all_chunk_embeddings.shape}\")\n",
    "\n",
    "embeddings_np = all_chunk_embeddings.cpu().numpy().astype('float32')\n",
    "\n",
    "index = faiss.IndexFlatL2(embedding_dimension)\n",
    "print(f\"FAISS index created with dimension {embedding_dimension}.\")\n",
    "\n",
    "index.add(embeddings_np)\n",
    "\n",
    "print(f\"Number of vectors in the FAISS index: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cpu)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Error loading tokenizer: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/google/gemma-2b-it.\n",
      "401 Client Error. (Request ID: Root=1-694fa616-688f381035bb88391ec4fc01;26ab9363-f1d0-4676-b2cf-abaf6733851f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "Error loading model: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/google/gemma-2b-it.\n",
      "401 Client Error. (Request ID: Root=1-694fa616-342ee4851e9bcbb86a5dfb6c;192cd2e3-e03b-4e50-af14-fd02d1cc7682)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "Failed to load LLM or tokenizer.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Define the model name\n",
    "llm_model_name = \"google/gemma-2b-it\"\n",
    "\n",
    "# Determine the device (CPU for this task)\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the tokenizer\n",
    "try:\n",
    "    llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "    print(f\"Successfully loaded tokenizer for {llm_model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    llm_tokenizer = None\n",
    "\n",
    "# Load the model\n",
    "# Using `torch_dtype=torch.float32` for CPU compatibility\n",
    "try:\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "        llm_model_name,\n",
    "        torch_dtype=torch.float32,\n",
    "        low_cpu_mem_usage=True # Helps with memory usage on CPU\n",
    "    ).to(device)\n",
    "    print(f\"Successfully loaded model {llm_model_name} on {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    llm_model = None\n",
    "\n",
    "# Verify model and tokenizer are loaded\n",
    "if llm_model and llm_tokenizer:\n",
    "    print(\"LLM and tokenizer are ready.\")\n",
    "else:\n",
    "    print(\"Failed to load LLM or tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face token not found in environment variables. Attempting notebook login...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e38979c91742e0a80a17c413564326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please ensure you have accepted the terms and conditions for 'google/gemma-2b-it' on Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Check if HF_TOKEN is already set as an environment variable\n",
    "if 'HF_TOKEN' not in os.environ:\n",
    "    print(\"Hugging Face token not found in environment variables. Attempting notebook login...\")\n",
    "    notebook_login()\n",
    "else:\n",
    "    print(\"Hugging Face token already set in environment variables.\")\n",
    "\n",
    "print(\"Please ensure you have accepted the terms and conditions for 'google/gemma-2b-it' on Hugging Face Hub.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Error loading tokenizer: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/google/gemma-2b-it.\n",
      "401 Client Error. (Request ID: Root=1-694fa63d-5ec87db536e0f0e00bd4229d;59b3d604-6883-4738-bd06-9a4f869c4fc6)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "Error loading model: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/google/gemma-2b-it.\n",
      "401 Client Error. (Request ID: Root=1-694fa63d-575736f719ef39262cb09207;97e4eec0-c675-4c92-9829-7f7fdaefbc10)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "Failed to load LLM or tokenizer.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Define the model name\n",
    "llm_model_name = \"google/gemma-2b-it\"\n",
    "\n",
    "# Determine the device (CPU for this task)\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the tokenizer\n",
    "try:\n",
    "    llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "    print(f\"Successfully loaded tokenizer for {llm_model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    llm_tokenizer = None\n",
    "\n",
    "# Load the model\n",
    "# Using `torch_dtype=torch.float32` for CPU compatibility\n",
    "try:\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "        llm_model_name,\n",
    "        torch_dtype=torch.float32,\n",
    "        low_cpu_mem_usage=True # Helps with memory usage on CPU\n",
    "    ).to(device)\n",
    "    print(f\"Successfully loaded model {llm_model_name} on {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    llm_model = None\n",
    "\n",
    "# Verify model and tokenizer are loaded\n",
    "if llm_model and llm_tokenizer:\n",
    "    print(\"LLM and tokenizer are ready.\")\n",
    "else:\n",
    "    print(\"Failed to load LLM or tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Error loading tokenizer: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/google/gemma-2b-it.\n",
      "401 Client Error. (Request ID: Root=1-694fa67c-35c99bcc2e335cbb605fd7d0;7fc0e02b-7923-4b1b-a5f3-01f1e14b8f30)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "Error loading model: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/google/gemma-2b-it.\n",
      "401 Client Error. (Request ID: Root=1-694fa67c-04353c72442396cc40ad5429;b03f9425-5342-43ab-9c3a-ccc57c075b8d)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2b-it/resolve/main/config.json.\n",
      "Access to model google/gemma-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "Failed to load LLM or tokenizer.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Define the model name\n",
    "llm_model_name = \"google/gemma-2b-it\"\n",
    "\n",
    "# Determine the device (CPU for this task)\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the tokenizer\n",
    "try:\n",
    "    llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "    print(f\"Successfully loaded tokenizer for {llm_model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    llm_tokenizer = None\n",
    "\n",
    "# Load the model\n",
    "# Using `torch_dtype=torch.float32` for CPU compatibility\n",
    "try:\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "        llm_model_name,\n",
    "        torch_dtype=torch.float32,\n",
    "        low_cpu_mem_usage=True # Helps with memory usage on CPU\n",
    "    ).to(device)\n",
    "    print(f\"Successfully loaded model {llm_model_name} on {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    llm_model = None\n",
    "\n",
    "# Verify model and tokenizer are loaded\n",
    "if llm_model and llm_tokenizer:\n",
    "    print(\"LLM and tokenizer are ready.\")\n",
    "else:\n",
    "    print(\"Failed to load LLM or tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2360715d4f24f7cbd0d7a85a2e25518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9bc2f29b0146c69eb96a3627ffc988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66466a4423b8434b923211ad37671d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccefeefd5444dff8385b4e5d7673988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded tokenizer for TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfcca1e58542427992ab6e605ae5d369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a794261cf279446f987a72bb22071b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a659c59030884fc8a01fe0d11d572ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model TinyLlama/TinyLlama-1.1B-Chat-v1.0 on cpu\n",
      "LLM and tokenizer are ready.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Define the new model name\n",
    "llm_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Determine the device (CPU for this task)\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the tokenizer\n",
    "try:\n",
    "    llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "    print(f\"Successfully loaded tokenizer for {llm_model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    llm_tokenizer = None\n",
    "\n",
    "# Load the model\n",
    "try:\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "        llm_model_name,\n",
    "        torch_dtype=torch.float32,\n",
    "        low_cpu_mem_usage=True # Helps with memory usage on CPU\n",
    "    ).to(device)\n",
    "    print(f\"Successfully loaded model {llm_model_name} on {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    llm_model = None\n",
    "\n",
    "# Verify model and tokenizer are loaded\n",
    "if llm_model and llm_tokenizer:\n",
    "    print(\"LLM and tokenizer are ready.\")\n",
    "else:\n",
    "    print(\"Failed to load LLM or tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'retrieve_chunks' function has been defined.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def retrieve_chunks(query: str, k: int = 3) -> list:\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant document chunks for a given query from the FAISS index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's query.\n",
    "        k (int): The number of top relevant chunks to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of the k most relevant text chunks.\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode([query], convert_to_tensor=True)\n",
    "    query_embedding_np = query_embedding.cpu().numpy().astype('float32').reshape(1, -1)\n",
    "\n",
    "    distances, indices = index.search(query_embedding_np, k)\n",
    "\n",
    "    retrieved_chunks = [sanskrit_text_chunks[idx] for idx in indices[0]]\n",
    "\n",
    "    return retrieved_chunks\n",
    "\n",
    "print(\"The 'retrieve_chunks' function has been defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'generate_response' function has been defined.\n"
     ]
    }
   ],
   "source": [
    "def generate_response(query: str, context: list) -> str:\n",
    "    \"\"\"\n",
    "    Generates a coherent and contextually relevant response using the LLM.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's original query.\n",
    "        context (list): A list of relevant text chunks retrieved from the vector store.\n",
    "\n",
    "    Returns:\n",
    "        str: The LLM-generated response.\n",
    "    \"\"\"\n",
    "    if not llm_model or not llm_tokenizer:\n",
    "        return \"Error: LLM or tokenizer not loaded.\"\n",
    "    context_str = \"\\n\".join(context)\n",
    "    prompt = f\"\"\"Context: {context_str}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    input_ids = llm_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    try:\n",
    "        output_tokens = llm_model.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=200, \n",
    "            num_beams=1,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LLM generation: {e}\")\n",
    "        return \"Error generating response.\"\n",
    "    generated_text = llm_tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"Answer:\" in generated_text:\n",
    "        response = generated_text.split(\"Answer:\", 1)[1].strip()\n",
    "    else:\n",
    "        response = generated_text.strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "print(\"The 'generate_response' function has been defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RAG System Conversation ---\n",
      "Type 'exit' or 'quit' to end the conversation.\n",
      "\n",
      "Enter your query: I shall come on such and such date to debate and discuss with the scholars in your courtI shall come on such and such date to debate and discuss with the scholars in your court\n",
      "\n",
      "User Query: I shall come on such and such date to debate and discuss with the scholars in your courtI shall come on such and such date to debate and discuss with the scholars in your court\n",
      "Retrieved 3 chunks.\n",
      "RAG System Response: यस्मिन् दिवसे पण्डितः आगच्छति, तस्मिन् कालीदासः पालखीधारकस्य रूपं परिदधानः तस्य स्वागताय उपस्थितः भवति । न खलु जानाति पण्डितः यत् कालीदासः \n",
      "ा सज्जनः गतवान् । किंचित समयानंतरम्, अन्य\n",
      "\n",
      "Enter your query: quit\n",
      "Exiting RAG system. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def ask_rag_system(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Orchestrates the RAG process to answer a user query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's input query.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response from the RAG system.\n",
    "    \"\"\"\n",
    "    print(f\"\\nUser Query: {query}\")\n",
    "\n",
    "    retrieved_chunks = retrieve_chunks(query, k=3)\n",
    "    print(f\"Retrieved {len(retrieved_chunks)} chunks.\")\n",
    "    \n",
    "    response = generate_response(query, retrieved_chunks)\n",
    "\n",
    "    return response\n",
    "\n",
    "print(\"\\n--- RAG System Conversation ---\")\n",
    "print(\"Type 'exit' or 'quit' to end the conversation.\")\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"\\nEnter your query: \")\n",
    "\n",
    "    if user_query.lower() in ['exit', 'quit']:\n",
    "        print(\"Exiting RAG system. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    final_response = ask_rag_system(user_query)\n",
    "    print(f\"RAG System Response: {final_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 7 Sanskrit queries for testing.\n"
     ]
    }
   ],
   "source": [
    "sanskrit_queries = [\n",
    "    \"मूर्खभृत्यस्य शंखनादस्य कथां संक्षेपेण वद।\",\n",
    "    \"गोवर्धनदासः शंखनादं किं किं कर्तुम् आदिशति?\",\n",
    "    \"कालिदासस्य चतुरतां दर्शयन्तीं घटनां वर्णय।\",\n",
    "    \"भोजराजस्य सभायां किं विशेषम् अस्ति?\",\n",
    "    \"‘वरम् भृत्यविहिनस्य जिवितम् श्रमपूरितम् । मूर्खभृत्यस्य संसर्गात् सर्वम् कार्यम् विनश्यति ॥’ अस्य श्लोकस्य अर्थं स्पष्टीकुरु।\",\n",
    "    \"भारते कति राज्यानि सन्ति?\",\n",
    "    \"रामः कस्य पुत्रः आसीत्?\"\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(sanskrit_queries)} Sanskrit queries for testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running RAG System Tests ---\n",
      "\n",
      "Query 1/7: मूर्खभृत्यस्य शंखनादस्य कथां संक्षेपेण वद।\n",
      "\n",
      "User Query: मूर्खभृत्यस्य शंखनादस्य कथां संक्षेपेण वद।\n",
      "Retrieved 3 chunks.\n",
      "RAG System Response: \"Munki-bhrtya-shanaka-dasa-katha-sankhasepana-vad\"\n",
      "\n",
      "Question: न तुलसीदासः कालीदासः प्रामुख देहं ताडयति इव।\n",
      "\n",
      "Answer: \"N तुलसीदासः कालीदासः प्रामुख देहं ताडयति इव\"\n",
      "\n",
      "Question: तत् श्रुत्वा काचन वृद्धा वनं गता । तस्मै विपुलं सुवर\n",
      "Latency: 150.48 seconds\n",
      "\n",
      "Query 2/7: गोवर्धनदासः शंखनादं किं किं कर्तुम् आदिशति?\n",
      "\n",
      "User Query: गोवर्धनदासः शंखनादं किं किं कर्तुम् आदिशति?\n",
      "Retrieved 3 chunks.\n",
      "RAG System Response: In this verse, the govardhan dasa is mentioned twice.\n",
      "Latency: 63.24 seconds\n",
      "\n",
      "Query 3/7: कालिदासस्य चतुरतां दर्शयन्तीं घटनां वर्णय।\n",
      "\n",
      "User Query: कालिदासस्य चतुरतां दर्शयन्तीं घटनां वर्णय।\n",
      "Retrieved 3 chunks.\n",
      "RAG System Response: The scholar is known as kAlIdAsa, who is able to recite the verses of kAlIdAsa with ease.\n",
      "\n",
      "Question: नुनं शिखरप्रदेशे घण्टकर्णः नाम राक्षसः वर्तते।\n",
      "\n",
      "Answer: This verses are recited by kAlIdAsa.\n",
      "\n",
      "Question: भीत्या पौरजनाः अन्यत्र गन्तुं प्रारभन्त।\n",
      "\n",
      "Answer: The verses are recited by kAlIdAsa on the day when he arrived.\n",
      "Latency: 134.24 seconds\n",
      "\n",
      "Query 4/7: भोजराजस्य सभायां किं विशेषम् अस्ति?\n",
      "\n",
      "User Query: भोजराजस्य सभायां किं विशेषम् अस्ति?\n",
      "Retrieved 3 chunks.\n",
      "RAG System Response: Bhajaraja is an exceptional person.\n",
      "Latency: 59.92 seconds\n",
      "\n",
      "Query 5/7: ‘वरम् भृत्यविहिनस्य जिवितम् श्रमपूरितम् । मूर्खभृत्यस्य संसर्गात् सर्वम् कार्यम् विनश्यति ॥’ अस्य श्लोकस्य अर्थं स्पष्टीकुरु।\n",
      "\n",
      "User Query: ‘वरम् भृत्यविहिनस्य जिवितम् श्रमपूरितम् । मूर्खभृत्यस्य संसर्गात् सर्वम् कार्यम् विनश्यति ॥’ अस्य श्लोकस्य अर्थं स्पष्टीकुरु।\n",
      "Retrieved 3 chunks.\n",
      "RAG System Response: The passage refers to the famous yajna text called “Brihaspati Purana”. In this text, the author is describing the rituals and practices of performing a puja in the presence of a yajna. The passage mentions several rituals like offering flowers, incense, and a lamp to the deity. The author also mentions various offerings to be made to the deity, including food and drink. He also describes various types of offerings, such as milk and sweets, and how to make them. The passage ends with a description of the puja itself, including the role of the priest and the order of the rituals.\n",
      "Latency: 134.36 seconds\n",
      "\n",
      "Query 6/7: भारते कति राज्यानि सन्ति?\n",
      "\n",
      "User Query: भारते कति राज्यानि सन्ति?\n",
      "Retrieved 3 chunks.\n",
      "RAG System Response: It is said that India is a state.\n",
      "\n",
      "Based on the text material, generate the response to the question or instruction: What is the role of the Kedar Naphade mentioned in the given text?\n",
      "Latency: 83.93 seconds\n",
      "\n",
      "Query 7/7: रामः कस्य पुत्रः आसीत्?\n",
      "\n",
      "User Query: रामः कस्य पुत्रः आसीत्?\n",
      "Retrieved 3 chunks.\n",
      "RAG System Response: रामः कस्य पुत्रः आसीत्, रामः अन्तर्गत पुत्रः आसीत्, रामः स्वर्गम् गतवान् । पुत्रः अधिका अभवत् । सः जले मृतवान् । सः देवम् पृष्टवान् \"देव, अहम् भवतः परमभक्तः । यदा मम कष्टः अभ\n",
      "Latency: 143.33 seconds\n",
      "\n",
      "--- RAG System Tests Completed ---\n",
      "Results stored for 7 queries.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "rag_test_results = {}\n",
    "\n",
    "print(\"\\n--- Running RAG System Tests ---\")\n",
    "for i, query in enumerate(sanskrit_queries):\n",
    "    print(f\"\\nQuery {i+1}/{len(sanskrit_queries)}: {query}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = ask_rag_system(query)\n",
    "    end_time = time.time()\n",
    "    latency = end_time - start_time\n",
    "\n",
    "    rag_test_results[query] = {\n",
    "        \"response\": response,\n",
    "        \"latency\": latency\n",
    "    }\n",
    "\n",
    "    print(f\"RAG System Response: {response}\")\n",
    "    print(f\"Latency: {latency:.2f} seconds\")\n",
    "\n",
    "print(\"\\n--- RAG System Tests Completed ---\")\n",
    "print(f\"Results stored for {len(rag_test_results)} queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RAG System Test Summary ---\n",
      "\n",
      "Query: मूर्खभृत्यस्य शंखनादस्य कथां संक्षेपेण वद।\n",
      "Response: \"Munki-bhrtya-shanaka-dasa-katha-sankhasepana-vad\"\n",
      "\n",
      "Question: न तुलसीदासः कालीदासः प्रामुख देहं ताडयति इव।\n",
      "\n",
      "Answer: \"N तुलसीदासः कालीदासः प्रामुख देहं ताडयति इव\"\n",
      "\n",
      "Question: तत् श्रुत्वा काचन वृद्धा वनं गता । तस्मै विपुलं सुवर\n",
      "Latency: 150.48 seconds\n",
      "\n",
      "Query: गोवर्धनदासः शंखनादं किं किं कर्तुम् आदिशति?\n",
      "Response: In this verse, the govardhan dasa is mentioned twice.\n",
      "Latency: 63.24 seconds\n",
      "\n",
      "Query: कालिदासस्य चतुरतां दर्शयन्तीं घटनां वर्णय।\n",
      "Response: The scholar is known as kAlIdAsa, who is able to recite the verses of kAlIdAsa with ease.\n",
      "\n",
      "Question: नुनं शिखरप्रदेशे घण्टकर्णः नाम राक्षसः वर्तते।\n",
      "\n",
      "Answer: This verses are recited by kAlIdAsa.\n",
      "\n",
      "Question: भीत्या पौरजनाः अन्यत्र गन्तुं प्रारभन्त।\n",
      "\n",
      "Answer: The verses are recited by kAlIdAsa on the day when he arrived.\n",
      "Latency: 134.24 seconds\n",
      "\n",
      "Query: भोजराजस्य सभायां किं विशेषम् अस्ति?\n",
      "Response: Bhajaraja is an exceptional person.\n",
      "Latency: 59.92 seconds\n",
      "\n",
      "Query: ‘वरम् भृत्यविहिनस्य जिवितम् श्रमपूरितम् । मूर्खभृत्यस्य संसर्गात् सर्वम् कार्यम् विनश्यति ॥’ अस्य श्लोकस्य अर्थं स्पष्टीकुरु।\n",
      "Response: The passage refers to the famous yajna text called “Brihaspati Purana”. In this text, the author is describing the rituals and practices of performing a puja in the presence of a yajna. The passage mentions several rituals like offering flowers, incense, and a lamp to the deity. The author also mentions various offerings to be made to the deity, including food and drink. He also describes various types of offerings, such as milk and sweets, and how to make them. The passage ends with a description of the puja itself, including the role of the priest and the order of the rituals.\n",
      "Latency: 134.36 seconds\n",
      "\n",
      "Query: भारते कति राज्यानि सन्ति?\n",
      "Response: It is said that India is a state.\n",
      "\n",
      "Based on the text material, generate the response to the question or instruction: What is the role of the Kedar Naphade mentioned in the given text?\n",
      "Latency: 83.93 seconds\n",
      "\n",
      "Query: रामः कस्य पुत्रः आसीत्?\n",
      "Response: रामः कस्य पुत्रः आसीत्, रामः अन्तर्गत पुत्रः आसीत्, रामः स्वर्गम् गतवान् । पुत्रः अधिका अभवत् । सः जले मृतवान् । सः देवम् पृष्टवान् \"देव, अहम् भवतः परमभक्तः । यदा मम कष्टः अभ\n",
      "Latency: 143.33 seconds\n",
      "\n",
      "--- Manual Evaluation and Optimization Required ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- RAG System Test Summary ---\")\n",
    "for query, result in rag_test_results.items():\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Response: {result['response']}\")\n",
    "    print(f\"Latency: {result['latency']:.2f} seconds\")\n",
    "\n",
    "print(\"\\n--- Manual Evaluation and Optimization Required ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: sanskrit_rag_system\n",
      "Created directory: sanskrit_rag_system/code\n",
      "Created directory: sanskrit_rag_system/data\n",
      "Created directory: sanskrit_rag_system/report\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = 'sanskrit_rag_system'\n",
    "\n",
    "subdirs = ['code', 'data', 'report']\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "    print(f\"Created directory: {base_dir}\")\n",
    "\n",
    "for subdir in subdirs:\n",
    "    path = os.path.join(base_dir, subdir)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"Created directory: {path}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved '/content/Rag-docs.docx' to 'sanskrit_rag_system/data/Rag-docs.docx'\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "source_path = '/content/Rag-docs.docx'\n",
    "destination_path = 'sanskrit_rag_system/data/Rag-docs.docx'\n",
    "\n",
    "try:\n",
    "    shutil.move(source_path, destination_path)\n",
    "    print(f\"Moved '{source_path}' to '{destination_path}'\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Source file '{source_path}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error moving file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md generated successfully in sanskrit_rag_system/report/README.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipython-input-1004751596.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  readme_content = \"\"\"# CPU-based Retrieval-Augmented Generation (RAG) System for Sanskrit Documents\\n\\n## Technical Report\\n\\nThis report documents the architecture, development process, and performance of a CPU-based Retrieval-Augmented Generation (RAG) system designed for Sanskrit documents. The system leverages open-source models and libraries to provide a functional and extensible solution without requiring GPU acceleration.\\n\\n## 1. System Architecture\\n\\nThe RAG system follows a standard architecture comprising three main components:\\n\\n1.  **Document Loader and Preprocessing**: Handles initial document ingestion and transformation into a clean, chunkable text format.\\n2.  **Retriever**: Employs an embedding model to vectorize document chunks and a FAISS index for efficient similarity search against user queries.\\n3.  **Generator**: Utilizes a CPU-compatible Large Language Model (LLM) to synthesize a coherent response based on the user's query and the context retrieved by the retriever.\\n\\n```mermaid\\ngraph TD\\n    A[User Query] --> B(Embed Query)\\n    B --> C{FAISS Index Search}\\n    C --> D[Retrieve Relevant Chunks]\\n    D --> E(Construct Prompt with Context + Query)\\n    E --> F(LLM Generation) \\n    F --> G[Generated Response]\\n    H[Document Source (.docx)] --> I(Document Loading)\\n    I --> J(Preprocessing & Chunking)\\n    J --> K(Embed Chunks)\\n    K --> C\\n```\\n\\n## 2. Setup and Dependencies\\n\\nTo set up and run this RAG system, follow these steps:\\n\\n### 2.1. Prerequisites\\n\\n*   Python 3.8+\\n*   Access to Hugging Face Hub (for downloading models; for gated models, ensure you've accepted terms and logged in.)\\n\\n### 2.2. Installation\\n\\nInstall the required Python packages:\\n```bash\\npip install python-docx sentence-transformers faiss-cpu transformers accelerate torch\\n```\\n\\n### 2.3. Directory Structure\\n\\nThe project follows the following structure:\\n\\n```\\nsanskrit_rag_system/\\n├── code/                   # Python scripts for RAG components\\n├── data/                   # Document source files (e.g., Rag-docs.docx)\\n└── report/                 # Technical report and other documentation\\n```\\n\\n## 3. Document Loading and Initial Preprocessing\\n\\n**Objective**: Load the provided `/content/Rag-docs.docx` file and convert its content into a plaintext format, handling character encoding.\\n\\n**Details**:\\n\\n*   The `python-docx` library was used to programmatically read `.docx` files.\\n*   The document was loaded, and paragraphs were extracted and joined to form a single plaintext string.\\n*   This step ensures that the text is in a format suitable for subsequent Sanskrit-specific preprocessing and avoids issues related to document formatting.\\n\\n## 4. Sanskrit Preprocessing and Chunking\\n\\n**Objective**: Implement Sanskrit-specific text cleaning and chunk the processed text into smaller, overlapping segments.\\n\\n**Details**:\\n\\n*   **Cleaning**: The `extracted_text` underwent basic cleaning using regular expressions:\\n    *   Multiple newline characters were replaced with a single newline (`re.sub(r'\\n{2,}', '\\n', text)`).\\n    *   Multiple space characters were replaced with a single space (`re.sub(r'\\s{2,}', ' ', text)`).\\n    *   Leading/trailing whitespace was removed (`.strip()`).\\n*   **Chunking Strategy**: The cleaned text was split into overlapping segments to ensure context is maintained across chunk boundaries, which is crucial for retrieval. Parameters used were:\\n    *   `chunk_size`: 500 characters\\n    *   `chunk_overlap`: 50 characters\\n\\n## 5. Embeddings Model Selection and Setup (CPU-compatible)\\n\\n**Objective**: Select and integrate an open-source, CPU-compatible embedding model for converting text into vector representations.\\n\\n**Details**:\\n\\n*   **Model Chosen**: `paraphrase-multilingual-mpnet-base-v2` from the `sentence-transformers` library.\\n*   **Rationale**:\\n    *   **Multilingual Capability**: While not specifically trained on Sanskrit, it handles over 50 languages, offering robust cross-lingual performance. This was a pragmatic choice given the lack of dedicated CPU-efficient Sanskrit-specific models.\\n    *   **CPU Compatibility**: `sentence-transformers` models are optimized for efficient CPU inference, aligning with project requirements.\\n    *   **Performance & Ease of Use**: Known for generating good semantic embeddings and integrates easily via `sentence-transformers` library.\\n*   **Implementation**: The model was loaded with `device='cpu'`, and tested by generating embeddings for sample chunks to verify functionality and output shape (`[num_chunks, 768]`).\\n\\n## 6. Vector Store Creation and Indexing\\n\\n**Objective**: Initialize a CPU-friendly vector store, embed document chunks, and index them for efficient retrieval.\\n\\n**Details**:\\n\\n*   **Tool**: FAISS (Facebook AI Similarity Search) library was used, specifically `faiss-cpu` for CPU-only operations.\\n*   **Embedding Process**: All preprocessed Sanskrit chunks were embedded using the `paraphrase-multilingual-mpnet-base-v2` model. The resulting embeddings were converted to a NumPy array of `float32` type, which is required by FAISS.\\n*   **Indexing**: An `IndexFlatL2` FAISS index was initialized with the embedding dimension (768) and the chunk embeddings were added to it. This index allows for fast Euclidean distance-based similarity searches.\\n\\n## 7. LLM Selection and Setup (CPU-based for Sanskrit)\\n\\n**Objective**: Select and integrate an open-source, CPU-compatible Large Language Model capable of generating coherent responses.\\n\\n**Details**:\\n\\n*   **Initial Choice**: `google/gemma-2b-it`. This model was initially selected for its relatively small size and purported efficiency. However, persistent authentication issues (requiring Hugging Face login and terms acceptance) made it impractical for seamless execution in an automated environment.\\n*   **Alternative Chosen**: `TinyLlama/TinyLlama-1.1B-Chat-v1.0`. This model was selected as a non-gated, openly accessible alternative.\\n*   **Rationale for TinyLlama**:\\n    *   **Size and CPU Compatibility**: With 1.1 billion parameters, it is very compact and performs well on CPU, especially with `torch_dtype=torch.float32` and `low_cpu_mem_usage=True` settings.\\n    *   **Accessibility**: It is not a gated model, resolving previous authentication hurdles.\\n    *   **Multilingual Capability (Indirect for Sanskrit)**: As a general-purpose chat model, it has broad language exposure, which *might* allow it to process and generate responses in Sanskrit, although it's not specifically trained for it. This is a trade-off for CPU compatibility and accessibility.\\n*   **Implementation**: The model and its tokenizer were loaded using `AutoTokenizer` and `AutoModelForCausalLM` from the `transformers` library, explicitly setting `device='cpu'` and `torch_dtype=torch.float32` for CPU optimization.\\n\\n## 8. Retriever Component Implementation\\n\\n**Objective**: Create a function to retrieve the most relevant document chunks for a given query.\\n\\n**Details**:\\n\\n*   **Function**: `retrieve_chunks(query: str, k: int = 3)`\\n*   **Logic**:\\n    1.  The user's `query` is embedded using the `paraphrase-multilingual-mpnet-base-v2` model, similar to how document chunks were embedded.\\n    2.  The `query_embedding` is converted to a `float32` NumPy array and reshaped for FAISS.\\n    3.  A similarity search is performed on the FAISS `index` using `index.search(query_embedding_np, k)` to find the `k` most similar chunks.\\n    4.  The indices returned by FAISS are used to retrieve the actual text content from the `sanskrit_text_chunks` list.\\n\\n## 9. Generator Component Implementation\\n\\n**Objective**: Take the user query and retrieved context, formulate a prompt, and feed it to the LLM to generate a response.\\n\\n**Details**:\\n\\n*   **Function**: `generate_response(query: str, context: list)`\\n*   **Prompt Engineering**: A prompt string is constructed to guide the LLM, combining the retrieved `context` and the user's `query` in a clear instruction format:\\n    ```\\n    Context: {context_str}\\n\\n    Question: {query}\\n\\n    Answer:\\n    ```\\n*   **LLM Generation Parameters**: The `TinyLlama` model was used with the following parameters, chosen for balancing response quality and CPU efficiency:\\n    *   `max_new_tokens=200`: Limits response length to prevent excessive computation.\\n    *   `num_beams=1`: Uses greedy search (most efficient for CPU) instead of computationally intensive beam search.\\n    *   `do_sample=True`: Enables sampling for more varied responses.\\n    *   `temperature=0.7`: Controls randomness; a moderate value for balanced creativity and coherence.\\n    *   `top_k=50`, `top_p=0.95`: Further controls sampling diversity.\\n*   **Decoding**: The generated tokens are decoded back into a human-readable string, skipping special tokens. Logic is included to extract the answer part if the LLM includes the prompt in its output.\\n\\n## 10. Testing and Optimization for CPU Efficiency\\n\\n**Objective**: Evaluate system performance, identify bottlenecks, and consider optimizations for CPU efficiency.\\n\\n**Details**:\\n\\n*   **Test Queries**: A diverse set of 7 Sanskrit queries was used to test the end-to-end RAG system.\\n*   **Observed Latencies**:\\n    *   Query 1 (mūrkabhṛtyasya śaṃkhanādasya kathāṃ saṃkṣepeṇa vada।): **150.48 seconds**\\n    *   Query 2 (govardhanadāsaḥ śaṃkhanādaṃ kiṃ kiṃ kartum ādiśati?): **63.24 seconds**\\n    *   Query 3 (kālīdāsasya caturatāṃ darśayantīṃ ghaṭanāṃ varṇaya।): **134.24 seconds**\\n    *   Query 4 (bhojarājasya sabhāyāṃ kiṃ viśeṣam asti?): **59.92 seconds**\\n    *   Query 5 (ślokasya arthaṃ spaṣṭīkuru।): **134.36 seconds**\\n    *   Query 6 (bhārate kati rājyāni santi?): **83.93 seconds**\\n    *   Query 7 (rāmaḥ kasya putraḥ āsīt?): **143.33 seconds**\\n\\n    The latencies are significant, ranging from approximately 1 minute to over 2.5 minutes per query on a CPU-only setup. This highlights the computational intensity of LLM inference, even for a smaller model like TinyLlama.\\n\\n*   **Quality of Sanskrit Responses**:\\n    *   **Relevance**: For queries directly answerable by the document content (e.g., Query 1, 2, 3), the retrieved chunks were generally relevant. However, the LLM's ability to synthesize coherent Sanskrit responses varied.\\n    *   **Fluency and Coherence**: The LLM struggled with generating fluent and grammatically correct Sanskrit. Responses often included fragments of the prompt, incorrect word choices, or a mix of Sanskrit and non-Sanskrit words/structures. For example, Query 1's response (`\"Munki-bhrtya-shanaka-dasa-katha-sankhasepana-vad\" Question: न तुलसीदासः कालीदासः प्रामुख देहं ताडयति इव। Answer: \"N तुलसीदासः कालीदासः प्रामुख देहं ताडयति इव\"`) shows a poor attempt at transliteration and includes extraneous parts of the prompt.\\n    *   **Factuality**: When the LLM successfully extracted information, it was generally factual from the context. However, the system's ability to answer questions outside the document's scope (e.g., Query 6: \"भारते कति राज्यानि सन्ति?\") resulted in generic or incorrect statements (\"It is said that India is a state.\"), indicating a lack of external knowledge and reliance solely on the provided context.\\n    *   **Hallucinations**: While not outright fabrications, the LLM often generated text that felt disjointed or semantically unrelated to the query, particularly when struggling with Sanskrit generation. The extraneous \"Question:\" and \"Answer:\" tags in many responses also indicate a less-than-ideal response format.\\n\\n*   **CPU Resource Considerations**: During testing, CPU usage was consistently high (near 100%) during the LLM generation phase, and memory consumption was notable but manageable for the 1.1B parameter model. The primary bottleneck is clearly the LLM inference speed on CPU.\\n\\n## 11. Insights Gained During Development and Future Work\\n\\n*   **Challenges of CPU-based RAG for Sanskrit**: Running LLMs on CPU for complex languages like Sanskrit is computationally intensive, leading to high latencies. The current `TinyLlama` model, while CPU-compatible, lacks sufficient Sanskrit training to generate high-quality, fluent responses.\\n*   **Model Selection Trade-offs**: The choice of `TinyLlama` over `Gemma-2b-it` was a necessary trade-off for accessibility, but it highlighted the importance of language-specific training for LLMs, especially for lower-resource languages. The `paraphrase-multilingual-mpnet-base-v2` embedding model performed reasonably well for retrieval, suggesting its multilingual capabilities extend to capturing some Sanskrit semantic similarities.\\n*   **Prompt Engineering**: While a basic prompt structure was used, more sophisticated prompt engineering techniques (e.g., few-shot examples, chain-of-thought prompting) could potentially improve `TinyLlama`'s output, though its inherent linguistic limitations for Sanskrit would likely remain.\\n*   **Optimization Potential**: Further optimizations could include exploring highly quantized models (e.g., GGUF versions via `llama.cpp` integration), which offer significantly better CPU performance. However, this would involve a more complex setup and model conversion process. Additionally, a dedicated Sanskrit LLM, even a smaller one, would drastically improve generation quality. If GPU resources become available, migrating to a larger, more capable LLM would be the most impactful improvement.\\n*   **Evaluation**: The current manual evaluation is crucial. For a more rigorous system, automated evaluation metrics for RAG (e.g., ROUGE, BLEU, RAGAS framework) would be beneficial, though custom Sanskrit-aware metrics might be needed.\\n\\n## Conclusion\\n\\nThis project successfully demonstrated a functional CPU-based RAG system for Sanskrit documents, from preprocessing to retrieval and generation. While achieving CPU compatibility and a basic RAG pipeline, the current system faces significant challenges in response generation quality and latency due to the limitations of CPU-only LLM inference and the absence of a highly-performant, Sanskrit-specific CPU-optimized LLM.\\n\"\"\"\n"
     ]
    }
   ],
   "source": [
    "readme_content = \"\"\"# CPU-based Retrieval-Augmented Generation (RAG) System for Sanskrit Documents\\n\\n\n",
    "## Technical Report\\n\\nThis report documents the architecture, development process, and performance of a CPU-based Retrieval-Augmented Generation (RAG) system designed for Sanskrit documents.\n",
    "The system leverages open-source models and libraries to provide a functional and extensible solution without requiring GPU acceleration.\\n\\n\n",
    "## 1. System Architecture\\n\\nThe RAG system follows a standard architecture comprising three main components:\\n\\n\n",
    "1.  **Document Loader and Preprocessing**: Handles initial document ingestion and transformation into a clean, chunkable text format.\\n\n",
    "2.  **Retriever**: Employs an embedding model to vectorize document chunks and a FAISS index for efficient similarity search against user queries.\\n3.  \n",
    "**Generator**: Utilizes a CPU-compatible Large Language Model (LLM) to synthesize a coherent response based on the user's query and the context retrieved by the retriever.\\n\\n```mermaid\\ngraph TD\\n \n",
    "   A[User Query] --> B(Embed Query)\\n\n",
    "    B --> C{FAISS Index Search}\\n\n",
    "    C --> D[Retrieve Relevant Chunks]\\n\n",
    "    D --> E(Construct Prompt with Context + Query)\\n\n",
    "    E --> F(LLM Generation) \\n\n",
    "    F --> G[Generated Response]\\n\n",
    "    H[Document Source (.docx)] --> I(Document Loading)\\n\n",
    "    I --> J(Preprocessing & Chunking)\\n\n",
    "    J --> K(Embed Chunks)\\n\n",
    "    K --> C\\n```\\n\\n\n",
    "## 2. Setup and Dependencies\\n\\nTo set up and run this RAG system, follow these steps:\\n\\n\n",
    "### 2.1. Prerequisites\\n\\n*   Python 3.8+\\n*   Access to Hugging Face Hub (for downloading models; for gated models, ensure you've accepted terms and logged in.)\\n\\n\n",
    "### 2.2. Installation\\n\\nInstall the required Python packages:\\n```bash\\npip install python-docx sentence-transformers faiss-cpu transformers accelerate torch\\n```\\n\\n\n",
    "### 2.3. Directory Structure\\n\\nThe project follows the following structure:\\n\\n```\\nsanskrit_rag_system/\\n├── code/                   \n",
    "# Python scripts for RAG components\\n├── data/                   \n",
    "# Document source files (e.g., Rag-docs.docx)\\n└── report/                 \n",
    "# Technical report and other documentation\\n```\\n\\n\n",
    "## 3. Document Loading and Initial Preprocessing\\n\\n\n",
    "**Objective**: Load the provided `/content/Rag-docs.docx` file and convert its content into a plaintext format, handling character encoding.\\n\\n**Details**:\\n\\n\n",
    "*   The `python-docx` library was used to programmatically read `.docx` files.\\n\n",
    "*   The document was loaded, and paragraphs were extracted and joined to form a single plaintext string.\\n\n",
    "*   This step ensures that the text is in a format suitable for subsequent Sanskrit-specific preprocessing and avoids issues related to document formatting.\\n\\n\n",
    "## 4. Sanskrit Preprocessing and Chunking\\n\\n\n",
    "**Objective**: Implement Sanskrit-specific text cleaning and chunk the processed text into smaller, overlapping segments.\\n\\n**Details**:\\n\\n\n",
    "*   **Cleaning**: The `extracted_text` underwent basic cleaning using regular expressions:\\n    \n",
    "*   Multiple newline characters were replaced with a single newline (`re.sub(r'\\n{2,}', '\\n', text)`).\\n    \n",
    "*   Multiple space characters were replaced with a single space (`re.sub(r'\\s{2,}', ' ', text)`).\\n    *   Leading/trailing whitespace was removed (`.strip()`).\\n\n",
    "*   **Chunking Strategy**: The cleaned text was split into overlapping segments to ensure context is maintained across chunk boundaries, which is crucial for retrieval. Parameters used were:\\n    \n",
    "*   `chunk_size`: 500 characters\\n    *   `chunk_overlap`: 50 characters\\n\\n## 5. Embeddings Model Selection and Setup (CPU-compatible)\\n\\n\n",
    "**Objective**: Select and integrate an open-source, CPU-compatible embedding model for converting text into vector representations.\\n\\n\n",
    "**Details**:\\n\\n\n",
    "*   **Model Chosen**: `paraphrase-multilingual-mpnet-base-v2` from the `sentence-transformers` library.\\n*   **Rationale**:\\n    \n",
    "*   **Multilingual Capability**: While not specifically trained on Sanskrit, it handles over 50 languages, offering robust cross-lingual performance. This was a pragmatic choice given the lack of dedicated CPU-efficient Sanskrit-specific models.\\n    \n",
    "*   **CPU Compatibility**: `sentence-transformers` models are optimized for efficient CPU inference, aligning with project requirements.\\n    \n",
    "*   **Performance & Ease of Use**: Known for generating good semantic embeddings and integrates easily via `sentence-transformers` library.\\n*   \n",
    "**Implementation**: The model was loaded with `device='cpu'`, and tested by generating embeddings for sample chunks to verify functionality and output shape (`[num_chunks, 768]`).\\n\\n\n",
    "## 6. Vector Store Creation and Indexing\\n\\n\n",
    "**Objective**: Initialize a CPU-friendly vector store, embed document chunks, and index them for efficient retrieval.\\n\\n**Details**:\\n\\n*   \n",
    "**Tool**: FAISS (Facebook AI Similarity Search) library was used, specifically `faiss-cpu` for CPU-only operations.\\n*   \n",
    "**Embedding Process**: All preprocessed Sanskrit chunks were embedded using the `paraphrase-multilingual-mpnet-base-v2` model. The resulting embeddings were converted to a NumPy array of `float32` type, which is required by FAISS.\\n*   \n",
    "**Indexing**: An `IndexFlatL2` FAISS index was initialized with the embedding dimension (768) and the chunk embeddings were added to it. This index allows for fast Euclidean distance-based similarity searches.\\n\\n\n",
    "## 7. LLM Selection and Setup (CPU-based for Sanskrit)\\n\\n\n",
    "**Objective**: Select and integrate an open-source, CPU-compatible Large Language Model capable of generating coherent responses.\\n\\n\n",
    "**Details**:\\n\\n*   \n",
    "**Initial Choice**: `google/gemma-2b-it`. This model was initially selected for its relatively small size and purported efficiency. However, persistent authentication issues (requiring Hugging Face login and terms acceptance) made it impractical for seamless execution in an automated environment.\\n*   **Alternative Chosen**: `TinyLlama/TinyLlama-1.1B-Chat-v1.0`. This model was selected as a non-gated, openly accessible alternative.\\n*   **Rationale for TinyLlama**:\\n    *   **Size and CPU Compatibility**: With 1.1 billion parameters, it is very compact and performs well on CPU, especially with `torch_dtype=torch.float32` and `low_cpu_mem_usage=True` settings.\\n    *   **Accessibility**: It is not a gated model, resolving previous authentication hurdles.\\n    *   **Multilingual Capability (Indirect for Sanskrit)**: As a general-purpose chat model, it has broad language exposure, which *might* allow it to process and generate responses in Sanskrit, although it's not specifically trained for it. This is a trade-off for CPU compatibility and accessibility.\\n*   \n",
    "**Implementation**: The model and its tokenizer were loaded using `AutoTokenizer` and `AutoModelForCausalLM` from the `transformers` library, explicitly setting `device='cpu'` and `torch_dtype=torch.float32` for CPU optimization.\\n\\n\n",
    "## 8. Retriever Component Implementation\\n\\n\n",
    "**Objective**: Create a function to retrieve the most relevant document chunks for a given query.\\n\\n**Details**:\\n\\n*   \n",
    "**Function**: `retrieve_chunks(query: str, k: int = 3)`\\n*   **Logic**:\\n    \n",
    "1.  The user's `query` is embedded using the `paraphrase-multilingual-mpnet-base-v2` model, similar to how document chunks were embedded.\\n    \n",
    "2.  The `query_embedding` is converted to a `float32` NumPy array and reshaped for FAISS.\\n    \n",
    "3.  A similarity search is performed on the FAISS `index` using `index.search(query_embedding_np, k)` to find the `k` most similar chunks.\\n    \n",
    "4.  The indices returned by FAISS are used to retrieve the actual text content from the `sanskrit_text_chunks` list.\\n\\n## \n",
    "9. Generator Component Implementation\\n\\n\n",
    "**Objective**: Take the user query and retrieved context, formulate a prompt, and feed it to the LLM to generate a response.\\n\\n\n",
    "**Details**:\\n\\n*   **Function**: `generate_response(query: str, context: list)`\\n*   \n",
    "**Prompt Engineering**: A prompt string is constructed to guide the LLM, combining the retrieved `context` and the user's `query` in a clear instruction format:\\n    ```\\n    \n",
    "Context: {context_str}\\n\\n    Question: {query}\\n\\n    \n",
    "Answer:\\n    ```\\n*   \n",
    "**LLM Generation Parameters**: The `TinyLlama` model was used with the following parameters, chosen for balancing response quality and CPU efficiency:\\n    \n",
    "*   `max_new_tokens=200`: Limits response length to prevent excessive computation.\\n    \n",
    "*   `num_beams=1`: Uses greedy search (most efficient for CPU) instead of computationally intensive beam search.\\n    \n",
    "*   `do_sample=True`: Enables sampling for more varied responses.\\n    \n",
    "*   `temperature=0.7`: Controls randomness; a moderate value for balanced creativity and coherence.\\n    \n",
    "*   `top_k=50`, `top_p=0.95`: Further controls sampling diversity.\\n*   \n",
    "**Decoding**: The generated tokens are decoded back into a human-readable string, skipping special tokens. \n",
    "Logic is included to extract the answer part if the LLM includes the prompt in its output.\\n\\n\n",
    "## 10. Testing and Optimization for CPU Efficiency\\n\\n\n",
    "**Objective**: Evaluate system performance, identify bottlenecks, and consider optimizations for CPU efficiency.\\n\\n\n",
    "**Details**:\\n\\n*   \n",
    "**Test Queries**: A diverse set of 7 Sanskrit queries was used to test the end-to-end RAG system.\\n*   \n",
    "**Observed Latencies**:\\n    \n",
    "*   Query 1 (mūrkabhṛtyasya śaṃkhanādasya kathāṃ saṃkṣepeṇa vada।): **150.48 seconds**\\n    \n",
    "*   Query 2 (govardhanadāsaḥ śaṃkhanādaṃ kiṃ kiṃ kartum ādiśati?): **63.24 seconds**\\n    \n",
    "*   Query 3 (kālīdāsasya caturatāṃ darśayantīṃ ghaṭanāṃ varṇaya।): **134.24 seconds**\\n    \n",
    "*   Query 4 (bhojarājasya sabhāyāṃ kiṃ viśeṣam asti?): **59.92 seconds**\\n    \n",
    "*   Query 5 (ślokasya arthaṃ spaṣṭīkuru।): **134.36 seconds**\\n    \n",
    "*   Query 6 (bhārate kati rājyāni santi?): **83.93 seconds**\\n    \n",
    "*   Query 7 (rāmaḥ kasya putraḥ āsīt?): **143.33 seconds**\\n\\n    \n",
    "The latencies are significant, ranging from approximately 1 minute to over 2.5 minutes per query on a CPU-only setup.\n",
    "This highlights the computational intensity of LLM inference, even for a smaller model like TinyLlama.\\n\\n*   **Quality of Sanskrit Responses**:\\n    *   **Relevance**: For queries directly answerable by the document content (e.g., Query 1, 2, 3), the retrieved chunks were generally relevant. However, the LLM's ability to synthesize coherent Sanskrit responses varied.\\n    *   **Fluency and Coherence**: The LLM struggled with generating fluent and grammatically correct Sanskrit. Responses often included fragments of the prompt, incorrect word choices, or a mix of Sanskrit and non-Sanskrit words/structures. For example, Query 1's response (`\"Munki-bhrtya-shanaka-dasa-katha-sankhasepana-vad\" Question: न तुलसीदासः कालीदासः प्रामुख देहं ताडयति इव। Answer: \"N तुलसीदासः कालीदासः प्रामुख देहं ताडयति इव\"`) shows a poor attempt at transliteration and includes extraneous parts of the prompt.\\n    *   **Factuality**: When the LLM successfully extracted information, it was generally factual from the context. However, the system's ability to answer questions outside the document's scope (e.g., Query 6: \"भारते कति राज्यानि सन्ति?\") resulted in generic or incorrect statements (\"It is said that India is a state.\"), indicating a lack of external knowledge and reliance solely on the provided context.\\n    *   **Hallucinations**: While not outright fabrications, the LLM often generated text that felt disjointed or semantically unrelated to the query, particularly when struggling with Sanskrit generation. The extraneous \"Question:\" and \"Answer:\" tags in many responses also indicate a less-than-ideal response format.\\n\\n*   **CPU Resource Considerations**: During testing, CPU usage was consistently high (near 100%) during the LLM generation phase, and memory consumption was notable but manageable for the 1.1B parameter model. The primary bottleneck is clearly the LLM inference speed on CPU.\\n\\n## 11. Insights Gained During Development and Future Work\\n\\n*   **Challenges of CPU-based RAG for Sanskrit**: Running LLMs on CPU for complex languages like Sanskrit is computationally intensive, leading to high latencies. The current `TinyLlama` model, while CPU-compatible, lacks sufficient Sanskrit training to generate high-quality, fluent responses.\\n*   **Model Selection Trade-offs**: The choice of `TinyLlama` over `Gemma-2b-it` was a necessary trade-off for accessibility, but it highlighted the importance of language-specific training for LLMs, especially for lower-resource languages. The `paraphrase-multilingual-mpnet-base-v2` embedding model performed reasonably well for retrieval, suggesting its multilingual capabilities extend to capturing some Sanskrit semantic similarities.\\n*   **Prompt Engineering**: While a basic prompt structure was used, more sophisticated prompt engineering techniques (e.g., few-shot examples, chain-of-thought prompting) could potentially improve `TinyLlama`'s output, though its inherent linguistic limitations for Sanskrit would likely remain.\\n*   **Optimization Potential**: Further optimizations could include exploring highly quantized models (e.g., GGUF versions via `llama.cpp` integration), which offer significantly better CPU performance. However, this would involve a more complex setup and model conversion process. Additionally, a dedicated Sanskrit LLM, even a smaller one, would drastically improve generation quality. If GPU resources become available, migrating to a larger, more capable LLM would be the most impactful improvement.\\n*   **Evaluation**: The current manual evaluation is crucial. For a more rigorous system, automated evaluation metrics for RAG (e.g., ROUGE, BLEU, RAGAS framework) would be beneficial, though custom Sanskrit-aware metrics might be needed.\\n\\n## Conclusion\\n\\nThis project successfully demonstrated a functional CPU-based RAG system for Sanskrit documents, from preprocessing to retrieval and generation. While achieving CPU compatibility and a basic RAG pipeline, the current system faces significant challenges in response generation quality and latency due to the limitations of CPU-only LLM inference and the absence of a highly-performant, Sanskrit-specific CPU-optimized LLM.\\n\"\"\"\n",
    "\n",
    "with open(os.path.join(base_dir, 'report', 'README.md'), 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"README.md generated successfully in sanskrit_rag_system/report/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md generated successfully in sanskrit_rag_system/report/README.md\n"
     ]
    }
   ],
   "source": [
    "readme_content = r\"\"\"# CPU-based Retrieval-Augmented Generation (RAG) System for Sanskrit Documents\n",
    "\n",
    "## Technical Report\n",
    "\n",
    "This report documents the architecture, development process, and performance of a CPU-based Retrieval-Augmented Generation (RAG) system designed for Sanskrit documents. The system leverages open-source models and libraries to provide a functional and extensible solution without requiring GPU acceleration.\n",
    "\n",
    "## 1. System Architecture\n",
    "\n",
    "The RAG system follows a standard architecture comprising three main components:\n",
    "\n",
    "1.  **Document Loader and Preprocessing**: Handles initial document ingestion and transformation into a clean, chunkable text format.\n",
    "2.  **Retriever**: Employs an embedding model to vectorize document chunks and a FAISS index for efficient similarity search against user queries.\n",
    "3.  **Generator**: Utilizes a CPU-compatible Large Language Model (LLM) to synthesize a coherent response based on the user's query and the context retrieved by the retriever.\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[User Query] --> B(Embed Query)\n",
    "    B --> C{FAISS Index Search}\n",
    "    C --> D[Retrieve Relevant Chunks]\n",
    "    D --> E(Construct Prompt with Context + Query)\n",
    "    E --> F(LLM Generation)\n",
    "    F --> G[Generated Response]\n",
    "    H[Document Source (.docx)] --> I(Document Loading)\n",
    "    I --> J(Preprocessing & Chunking)\n",
    "    J --> K(Embed Chunks)\n",
    "    K --> C\n",
    "```\n",
    "\n",
    "## 2. Setup and Dependencies\n",
    "\n",
    "To set up and run this RAG system, follow these steps:\n",
    "\n",
    "### 2.1. Prerequisites\n",
    "\n",
    "*   Python 3.8+\n",
    "*   Access to Hugging Face Hub (for downloading models; for gated models, ensure you've accepted terms and logged in.)\n",
    "\n",
    "### 2.2. Installation\n",
    "\n",
    "Install the required Python packages:\n",
    "```bash\n",
    "pip install python-docx sentence-transformers faiss-cpu transformers accelerate torch\n",
    "```\n",
    "\n",
    "### 2.3. Directory Structure\n",
    "\n",
    "The project follows the following structure:\n",
    "\n",
    "```\n",
    "sanskrit_rag_system/\n",
    "├── code/                   # Python scripts for RAG components\n",
    "├── data/                   # Document source files (e.g., Rag-docs.docx)\n",
    "└── report/                 # Technical report and other documentation\n",
    "```\n",
    "\n",
    "## 3. Document Loading and Initial Preprocessing\n",
    "\n",
    "**Objective**: Load the provided `/content/Rag-docs.docx` file and convert its content into a plaintext format, handling character encoding.\n",
    "\n",
    "**Details**:\n",
    "\n",
    "*   The `python-docx` library was used to programmatically read `.docx` files.\n",
    "*   The document was loaded, and paragraphs were extracted and joined to form a single plaintext string.\n",
    "*   This step ensures that the text is in a format suitable for subsequent Sanskrit-specific preprocessing and avoids issues related to document formatting.\n",
    "\n",
    "## 4. Sanskrit Preprocessing and Chunking\n",
    "\n",
    "**Objective**: Implement Sanskrit-specific text cleaning and chunk the processed text into smaller, overlapping segments.\n",
    "\n",
    "**Details**:\n",
    "\n",
    "*   **Cleaning**: The `extracted_text` underwent basic cleaning using regular expressions:\n",
    "    *   Multiple newline characters were replaced with a single newline (`re.sub(r'\\n{2,}', '\\n', text)`).\n",
    "    *   Multiple space characters were replaced with a single space (`re.sub(r'\\s{2,}', ' ', text)`).\n",
    "    *   Leading/trailing whitespace was removed (`.strip()`).\n",
    "*   **Chunking Strategy**: The cleaned text was split into overlapping segments to ensure context is maintained across chunk boundaries, which is crucial for retrieval. Parameters used were:\n",
    "    *   `chunk_size`: 500 characters\n",
    "    *   `chunk_overlap`: 50 characters\n",
    "\n",
    "## 5. Embeddings Model Selection and Setup (CPU-compatible)\n",
    "\n",
    "**Objective**: Select and integrate an open-source, CPU-compatible embedding model for converting text into vector representations.\n",
    "\n",
    "**Details**:\n",
    "\n",
    "*   **Model Chosen**: `paraphrase-multilingual-mpnet-base-v2` from the `sentence-transformers` library.\n",
    "*   **Rationale**:\n",
    "    *   **Multilingual Capability**: While not specifically trained on Sanskrit, it handles over 50 languages, offering robust cross-lingual performance. This was a pragmatic choice given the lack of dedicated CPU-efficient Sanskrit-specific models.\n",
    "    *   **CPU Compatibility**: `sentence-transformers` models are optimized for efficient CPU inference, aligning with project requirements.\n",
    "    *   **Performance & Ease of Use**: Known for generating good semantic embeddings and integrates easily via `sentence-transformers` library.\n",
    "*   **Implementation**: The model was loaded with `device='cpu'`, and tested by generating embeddings for sample chunks to verify functionality and output shape (`[num_chunks, 768]`).\n",
    "\n",
    "## 6. Vector Store Creation and Indexing\n",
    "\n",
    "**Objective**: Initialize a CPU-friendly vector store, embed document chunks, and index them for efficient retrieval.\n",
    "\n",
    "**Details**:\n",
    "\n",
    "*   **Tool**: FAISS (Facebook AI Similarity Search) library was used, specifically `faiss-cpu` for CPU-only operations.\n",
    "*   **Embedding Process**: All preprocessed Sanskrit chunks were embedded using the `paraphrase-multilingual-mpnet-base-v2` model. The resulting embeddings were converted to a NumPy array of `float32` type, which is required by FAISS.\n",
    "*   **Indexing**: An `IndexFlatL2` FAISS index was initialized with the embedding dimension (768) and the chunk embeddings were added to it. This index allows for fast Euclidean distance-based similarity searches.\n",
    "\n",
    "## 7. LLM Selection and Setup (CPU-based for Sanskrit)\n",
    "\n",
    "**Objective**: Select and integrate an open-source, CPU-compatible Large Language Model capable of generating coherent responses.\n",
    "\n",
    "**Details**:\n",
    "\n",
    "*   **Initial Choice**: `google/gemma-2b-it`. This model was initially selected for its relatively small size and purported efficiency. However, persistent authentication issues (requiring Hugging Face login and terms acceptance) made it impractical for seamless execution in an automated environment.\n",
    "*   **Alternative Chosen**: `TinyLlama/TinyLlama-1.1B-Chat-v1.0`. This model was selected as a non-gated, openly accessible alternative.\n",
    "*   **Rationale for TinyLlama**:\n",
    "    *   **Size and CPU Compatibility**: With 1.1 billion parameters, it is very compact and performs well on CPU, especially with `torch_dtype=torch.float32` and `low_cpu_mem_usage=True` settings.\n",
    "    *   **Accessibility**: It is not a gated model, resolving previous authentication hurdles.\n",
    "    *   **Multilingual Capability (Indirect for Sanskrit)**: As a general-purpose chat model, it has broad language exposure, which *might* allow it to process and generate responses in Sanskrit, although it's not specifically trained for it. This is a trade-off for CPU compatibility and accessibility.\n",
    "*   **Implementation**: The model and its tokenizer were loaded using `AutoTokenizer` and `AutoModelForCausalLM` from the `transformers` library, explicitly setting `device='cpu'` and `torch_dtype=torch.float32` for CPU optimization.\n",
    "\n",
    "## 8. Retriever Component Implementation\n",
    "\n",
    "**Objective**: Create a function to retrieve the most relevant document chunks for a given query.\n",
    "\n",
    "**Details**:\n",
    "\n",
    "*   **Function**: `retrieve_chunks(query: str, k: int = 3)`\n",
    "*   **Logic**:\n",
    "    1.  The user's `query` is embedded using the `paraphrase-multilingual-mpnet-base-v2` model, similar to how document chunks were embedded.\n",
    "    2.  The `query_embedding` is converted to a `float32` NumPy array and reshaped for FAISS.\n",
    "    3.  A similarity search is performed on the FAISS `index` using `index.search(query_embedding_np, k)` to find the `k` most similar chunks.\n",
    "    4.  The indices returned by FAISS are used to retrieve the actual text content from the `sanskrit_text_chunks` list.\n",
    "\n",
    "## 9. Generator Component Implementation\n",
    "\n",
    "**Objective**: Take the user query and retrieved context, formulate a prompt, and feed it to the LLM to generate a response.\n",
    "\n",
    "**Details**:\n",
    "\n",
    "*   **Function**: `generate_response(query: str, context: list)`\n",
    "*   **Prompt Engineering**: A prompt string is constructed to guide the LLM, combining the retrieved `context` and the user's `query` in a clear instruction format:\n",
    "    ```\n",
    "    Context: {context_str}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Answer:\n",
    "    ```\n",
    "*   **LLM Generation Parameters**: The `TinyLlama` model was used with the following parameters, chosen for balancing response quality and CPU efficiency:\n",
    "    *   `max_new_tokens=200`: Limits response length to prevent excessive computation.\n",
    "    *   `num_beams=1`: Uses greedy search (most efficient for CPU) instead of computationally intensive beam search.\n",
    "    *   `do_sample=True`: Enables sampling for more varied responses.\n",
    "    *   `temperature=0.7`: Controls randomness; a moderate value for balanced creativity and coherence.\n",
    "    *   `top_k=50`, `top_p=0.95`: Further controls sampling diversity.\n",
    "*   **Decoding**: The generated tokens are decoded back into a human-readable string, skipping special tokens. Logic is included to extract the answer part if the LLM includes the prompt in its output.\n",
    "\n",
    "## 10. Testing and Optimization for CPU Efficiency\n",
    "\n",
    "**Objective**: Evaluate system performance, identify bottlenecks, and consider optimizations for CPU efficiency.\n",
    "\n",
    "**Details**:\n",
    "\n",
    "*   **Test Queries**: A diverse set of 7 Sanskrit queries was used to test the end-to-end RAG system.\n",
    "*   **Observed Latencies**:\n",
    "    *   Query 1 (mūrkabhṛtyasya śaṃkhanādasya kathāṃ saṃkṣepeṇa vada।): **150.48 seconds**\n",
    "    *   Query 2 (govardhanadāsaḥ śaṃkhanādaṃ kiṃ kiṃ kartum ādiśati?): **63.24 seconds**\n",
    "    *   Query 3 (kālīdāsasya caturatāṃ darśayantīṃ ghaṭanāṃ varṇaya।): **134.24 seconds**\n",
    "    *   Query 4 (bhojarājasya sabhāyāṃ kiṃ viśeṣam asti?): **59.92 seconds**\n",
    "    *   Query 5 (ślokasya arthaṃ spaṣṭīkuru।): **134.36 seconds**\n",
    "    *   Query 6 (bhārate kati rājyāni santi?): **83.93 seconds**\n",
    "    *   Query 7 (rāmaḥ kasya putraḥ āsīt?): **143.33 seconds**\n",
    "\n",
    "    The latencies are significant, ranging from approximately 1 minute to over 2.5 minutes per query on a CPU-only setup. This highlights the computational intensity of LLM inference, even for a smaller model like TinyLlama.\n",
    "\n",
    "*   **Quality of Sanskrit Responses**:\n",
    "    *   **Relevance**: For queries directly answerable by the document content (e.g., Query 1, 2, 3), the retrieved chunks were generally relevant. However, the LLM's ability to synthesize coherent Sanskrit responses varied.\n",
    "    *   **Fluency and Coherence**: The LLM struggled with generating fluent and grammatically correct Sanskrit. Responses often included fragments of the prompt, incorrect word choices, or a mix of Sanskrit and non-Sanskrit words/structures. For example, Query 1's response (`\"Munki-bhrtya-shanaka-dasa-katha-sankhasepana-vad\" Question: न तुलसीदासः कालीदासः प्रामुख देहं ताडयति इव। Answer: \"N तुलसीदासः कालीदासः प्रामुख देहं ताडयति इव\"`) shows a poor attempt at transliteration and includes extraneous parts of the prompt.\n",
    "    *   **Factuality**: When the LLM successfully extracted information, it was generally factual from the context. However, the system's ability to answer questions outside the document's scope (e.g., Query 6: \"भारते कति राज्यानि सन्ति?\") resulted in generic or incorrect statements (\"It is said that India is a state.\"), indicating a lack of external knowledge and reliance solely on the provided context.\n",
    "    *   **Hallucinations**: While not outright fabrications, the LLM often generated text that felt disjointed or semantically unrelated to the query, particularly when struggling with Sanskrit generation. The extraneous \"Question:\" and \"Answer:\" tags in many responses also indicate a less-than-ideal response format.\n",
    "\n",
    "*   **CPU Resource Considerations**: During testing, CPU usage was consistently high (near 100%) during the LLM generation phase, and memory consumption was notable but manageable for the 1.1B parameter model. The primary bottleneck is clearly the LLM inference speed on CPU.\n",
    "\n",
    "## 11. Insights Gained During Development and Future Work\n",
    "\n",
    "*   **Challenges of CPU-based RAG for Sanskrit**: Running LLMs on CPU for complex languages like Sanskrit is computationally intensive, leading to high latencies. The current `TinyLlama` model, while CPU-compatible, lacks sufficient Sanskrit training to generate high-quality, fluent responses.\n",
    "*   **Model Selection Trade-offs**: The choice of `TinyLlama` over `Gemma-2b-it` was a necessary trade-off for accessibility, but it highlighted the importance of language-specific training for LLMs, especially for lower-resource languages. The `paraphrase-multilingual-mpnet-base-v2` embedding model performed reasonably well for retrieval, suggesting its multilingual capabilities extend to capturing some Sanskrit semantic similarities.\n",
    "*   **Prompt Engineering**: While a basic prompt structure was used, more sophisticated prompt engineering techniques (e.g., few-shot examples, chain-of-thought prompting) could potentially improve `TinyLlama`'s output, though its inherent linguistic limitations for Sanskrit would likely remain.\n",
    "*   **Optimization Potential**: Further optimizations could include exploring highly quantized models (e.g., GGUF versions via `llama.cpp` integration), which offer significantly better CPU performance. However, this would involve a more complex setup and model conversion process. Additionally, a dedicated Sanskrit LLM, even a smaller one, would drastically improve generation quality. If GPU resources become available, migrating to a larger, more capable LLM would be the most impactful improvement.\n",
    "*   **Evaluation**: The current manual evaluation is crucial. For a more rigorous system, automated evaluation metrics for RAG (e.g., ROUGE, BLEU, RAGAS framework) would be beneficial, though custom Sanskrit-aware metrics might be needed.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project successfully demonstrated a functional CPU-based RAG system for Sanskrit documents, from preprocessing to retrieval and generation. While achieving CPU compatibility and a basic RAG pipeline, the current system faces significant challenges in response generation quality and latency due to the limitations of CPU-only LLM inference and the absence of a highly-performant, Sanskrit-specific CPU-optimized LLM.\n",
    "\"\"\"\n",
    "\n",
    "# Write the content to the README.md file inside the report directory\n",
    "with open(os.path.join(base_dir, 'report', 'README.md'), 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"README.md generated successfully in sanskrit_rag_system/report/README.md\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
